# NVIDIA T4 Benchmark Data
# 10 runs per configuration

GPU_NAME = "T4"
GPU_DISPLAY_NAME = "NVIDIA T4"

# ==============================================================================
# Element-wise Addition
# ==============================================================================
ADD_FP32 = {
    "unfused": [5.19, 4.96, 5.50, 75.25, 712.89, 7104.91],
    "unfused_std": [0.37, 0.25, 0.84, 0.66, 4.72, 2.07],
    "fused": [2.50, 2.66, 2.80, 44.53, 422.95, 4214.65],
    "fused_std": [0.17, 0.46, 0.45, 0.57, 3.04, 5.40],
    "speedup": [2.08, 1.90, 1.97, 1.69, 1.69, 1.69],
    "speedup_std": [0.14, 0.26, 0.05, 0.02, 0.00, 0.01],
    "bw_unfused": [3.97, 41.43, 378.00, 272.18, 287.29, 288.25],
    "bw_unfused_std": [0.28, 2.06, 42.64, 2.36, 1.93, 0.08],
    "bw_fused": [4.93, 47.18, 446.60, 276.01, 290.55, 291.56],
    "bw_fused_std": [0.33, 6.28, 51.06, 3.50, 2.13, 0.38],
}

ADD_FP16 = {
    "unfused": [5.15, 5.53, 5.55, 45.06, 406.62, 3924.61],
    "unfused_std": [0.58, 0.72, 1.82, 8.60, 51.50, 119.26],
    "fused": [2.59, 2.67, 2.86, 24.53, 221.38, 2132.29],
    "fused_std": [0.32, 0.33, 0.94, 4.37, 23.39, 0.83],
    "speedup": [2.00, 2.08, 1.95, 1.83, 1.84, 1.84],
    "speedup_std": [0.10, 0.29, 0.05, 0.03, 0.03, 0.06],
    "bw_unfused": [2.00, 18.79, 194.60, 232.40, 254.62, 261.12],
    "bw_unfused_std": [0.18, 2.29, 34.85, 30.09, 24.43, 7.37],
    "bw_fused": [2.40, 23.27, 227.27, 255.54, 279.75, 288.14],
    "bw_fused_std": [0.25, 2.59, 41.28, 31.41, 23.33, 0.11],
}

# ==============================================================================
# Fused Multiply-Add (FMA)
# ==============================================================================
FMA_FP32 = {
    "unfused": [4.97, 5.09, 5.31, 88.56, 845.45, 8418.43],
    "unfused_std": [0.24, 0.37, 0.08, 0.79, 3.21, 4.05],
    "fused": [2.50, 2.42, 3.06, 57.47, 552.34, 5509.71],
    "fused_std": [0.23, 0.14, 0.49, 0.34, 1.98, 1.38],
    "speedup": [1.99, 2.10, 1.77, 1.54, 1.53, 1.53],
    "speedup_std": [0.15, 0.15, 0.19, 0.01, 0.00, 0.00],
    "bw_unfused": [4.96, 48.56, 462.78, 277.54, 290.69, 291.93],
    "bw_unfused_std": [0.23, 3.46, 6.43, 2.44, 1.11, 0.14],
    "bw_fused": [6.59, 67.82, 545.23, 285.07, 296.63, 297.37],
    "bw_fused_std": [0.56, 3.61, 62.77, 1.67, 1.07, 0.08],
}

FMA_FP16 = {
    "unfused": [5.17, 5.49, 5.83, 49.13, 446.56, 4277.47],
    "unfused_std": [0.58, 0.91, 1.77, 8.03, 61.56, 43.99],
    "fused": [2.65, 2.65, 3.04, 30.79, 287.99, 2781.39],
    "fused_std": [0.31, 0.33, 0.96, 2.69, 18.69, 0.82],
    "speedup": [1.96, 2.08, 1.92, 1.59, 1.55, 1.53],
    "speedup_std": [0.09, 0.28, 0.16, 0.10, 0.10, 0.02],
    "bw_unfused": [2.40, 22.85, 222.00, 254.45, 278.70, 287.30],
    "bw_unfused_std": [0.24, 3.24, 41.19, 29.51, 28.49, 2.88],
    "bw_fused": [3.13, 31.37, 283.83, 267.56, 285.38, 294.53],
    "bw_fused_std": [0.31, 3.45, 50.23, 19.25, 15.91, 0.09],
}

# ==============================================================================
# ReLU
# ==============================================================================
RELU_FP32 = {
    "unfused": [7.62, 7.77, 8.30, 106.46, 1003.39, 9951.51],
    "unfused_std": [0.61, 0.65, 2.03, 1.71, 4.52, 2.94],
    "fused": [2.54, 2.89, 2.93, 44.71, 421.51, 4203.80],
    "fused_std": [0.26, 0.73, 0.74, 0.59, 0.83, 1.68],
    "speedup": [3.01, 2.79, 2.84, 2.38, 2.38, 2.37],
    "speedup_std": [0.14, 0.46, 0.06, 0.05, 0.01, 0.00],
    "bw_unfused": [3.78, 37.14, 357.30, 269.39, 285.76, 288.12],
    "bw_unfused_std": [0.28, 2.99, 54.28, 4.22, 1.27, 0.09],
    "bw_fused": [4.88, 44.69, 434.90, 274.92, 291.52, 292.31],
    "bw_fused_std": [0.44, 9.31, 67.17, 3.60, 0.57, 0.12],
}

RELU_FP16 = {
    "unfused": [7.87, 7.62, 8.09, 62.35, 596.57, 5803.57],
    "unfused_std": [0.66, 0.56, 1.75, 8.78, 71.61, 25.79],
    "fused": [2.54, 2.68, 2.83, 24.73, 218.39, 2165.84],
    "fused_std": [0.20, 0.52, 0.61, 3.74, 8.55, 0.90],
    "speedup": [3.10, 2.91, 2.86, 2.53, 2.73, 2.68],
    "speedup_std": [0.24, 0.42, 0.07, 0.05, 0.20, 0.01],
    "bw_unfused": [1.83, 18.91, 182.14, 232.99, 242.72, 247.03],
    "bw_unfused_std": [0.15, 1.33, 25.68, 24.28, 22.36, 1.09],
    "bw_fused": [2.43, 23.52, 223.19, 252.32, 281.68, 283.68],
    "bw_fused_std": [0.18, 3.46, 31.21, 28.21, 10.04, 0.12],
}

# ==============================================================================
# Map Reduce - Naive Implementation
# (No data provided - fill in when available)
# ==============================================================================
MAP_REDUCE_NAIVE_FP32 = {
    "unfused": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    "unfused_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    "fused": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    "fused_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    "speedup": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    "speedup_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    "bw_unfused": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    "bw_unfused_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    "bw_fused": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    "bw_fused_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
}

MAP_REDUCE_NAIVE_FP16 = {
    "unfused": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    "unfused_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    "fused": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    "fused_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    "speedup": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    "speedup_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    "bw_unfused": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    "bw_unfused_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    "bw_fused": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    "bw_fused_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
}

# ==============================================================================
# Map Reduce - Block-level Implementation
# (No data provided - fill in when available)
# ==============================================================================
MAP_REDUCE_BLOCK_FP32 = {
    "unfused": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    "unfused_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    "fused": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    "fused_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    "speedup": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    "speedup_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    "bw_unfused": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    "bw_unfused_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    "bw_fused": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    "bw_fused_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
}

MAP_REDUCE_BLOCK_FP16 = {
    "unfused": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    "unfused_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    "fused": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    "fused_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    "speedup": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    "speedup_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    "bw_unfused": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    "bw_unfused_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    "bw_fused": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    "bw_fused_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
}

# ==============================================================================
# Plot configuration (y_max values for consistent scaling)
# T4 has lower bandwidth (~300 GB/s) than RTX 3080 (~760 GB/s)
# ==============================================================================
PLOT_CONFIG = {
    "add": {"speedup_y_max": 2.5, "bandwidth_y_max": 500},
    "fma": {"speedup_y_max": 2.5, "bandwidth_y_max": 500},
    "relu": {"speedup_y_max": 3.5, "bandwidth_y_max": 500},
    "map_reduce_naive": {"speedup_y_max": 1.75, "bandwidth_y_max": 20},
    "map_reduce_block": {"speedup_y_max": 2.25, "bandwidth_y_max": 300},
}